{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041fa5ee-f22d-458b-b4a2-d82f24432e19",
   "metadata": {},
   "source": [
    "# Loading the Dataset into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc0ba4-4905-4acf-b76d-f31b20dee30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df = pd.DataFrame({'text': newsgroups_data.data, 'target': newsgroups_data.target})\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00bad84-b9f9-4f72-89f7-69c14024f443",
   "metadata": {},
   "source": [
    "The dataset contains newsgroup documents categorized into 20 different topics, and is loaded into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce23f5a-1dab-469a-8ab5-77e3e7cd5930",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb55835-62bc-4709-a690-21a05f8118c0",
   "metadata": {},
   "source": [
    "Next step cleans the text data by removing stopwords, punctuation, and non-alphabetical characters, and converting to lowercase. nltk for stopwords and re for regular expressions is also used to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24bf5d-8907-4193-9573-da1230dfdf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punct and non-alphanum chars\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# preprocessing to the dataset\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# display clean text\n",
    "print(df['cleaned_text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad3f13-e751-484b-bcdc-6806a192d296",
   "metadata": {},
   "source": [
    "- nltk to download and apply stopwords.\n",
    "- preprocess_text function converts characters to lowercase, removes punctuation, and excludes stopwords.[1](#1)\n",
    "- Store the cleaned text in a new column called cleaned_text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b4293-9576-42b1-9e9b-fcb6534b475b",
   "metadata": {},
   "source": [
    "# Splitting the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc937cf-dd7b-4467-a152-f824039077f5",
   "metadata": {},
   "source": [
    "Next, the data is split into training and testnig sets to evaluate performance later in the process, using of train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0301a-b20f-49d3-badd-266a859069d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49928c71-73a4-40dd-8f69-d9eb2fb46b06",
   "metadata": {},
   "source": [
    "- Data was split so that 80% will be used for training and 20% for testing.\n",
    "- The train_test_split function shuffles the data and randomizes distribution of samples between training and testing sets.\n",
    "- random_state=42 is used to aid with reproducing by others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1e977-96f7-4180-ac3a-2d56e6d6ed6b",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20253b-400d-4e28-b40e-7c10fdb71a3e",
   "metadata": {},
   "source": [
    "Train Word2Vec embeddings using the cleaned text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073cb51-1444-476f-b864-58f412a9113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the text\n",
    "X_train_tokens = [text.split() for text in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=5, workers=4)\n",
    "word2vec_model.train(X_train_tokens, total_examples=len(X_train_tokens), epochs=10)\n",
    "\n",
    "# Get embeddings\n",
    "def get_average_word2vec(text, model, embedding_dim=100):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X_train_embeddings = np.array([get_average_word2vec(text, word2vec_model) for text in X_train])\n",
    "X_test_embeddings = np.array([get_average_word2vec(text, word2vec_model) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9862e-3b60-4b91-8f39-6a5ec61a1c7e",
   "metadata": {},
   "source": [
    "- Each document is tokenized by splitting it into words. Word2Vec requires tokenized text as input.\n",
    "\n",
    "- The Word2Vec model is trained on the tokenized training data. \"gensim\" gives the ability to specify the size of the word vectors (vector_size=100), the context window (window=5), and the minimum word count. min_count=5 means that words appearing less than 5 times are to be disregarded. \n",
    "\n",
    "- Once the model is trained, numerical representation is done by averaging word vectors for all words. If no words in a document are in the model's vocabulary then a zero is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c11422-38b8-4f90-a9be-ad5ed8d17f4d",
   "metadata": {},
   "source": [
    "# Building and Training a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff8642-e7b9-499f-9f01-49cd655f8928",
   "metadata": {},
   "source": [
    "Logistic Regression will classify the newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd3799-6fe6-4b82-bffe-4c65f1f0320f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_embeddings, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23967b-75bb-4495-b8f1-e537cfa22e69",
   "metadata": {},
   "source": [
    "- Logistic Regression is used along with with a high max_iter to help with convergence.\n",
    "- The model is trained using word embeddings as features and newsgroup categories as labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74b024-b040-47a7-a782-df333d3a4638",
   "metadata": {},
   "source": [
    "# Making Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc2b83-0e95-48e1-b30b-fd2dce3b3137",
   "metadata": {},
   "source": [
    "The trained model is used it to predict the newsgroup categories of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5d9c0-e3fb-4601-8866-4eee4f9c73a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the test set categories\n",
    "y_pred = clf.predict(X_test_embeddings)\n",
    "\n",
    "# Show a sample of predictions\n",
    "print(y_pred[:10])  # Show the first 10 predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cd41d-b1ca-492f-80e6-38eeb897b33a",
   "metadata": {},
   "source": [
    "- The first 10 predictions are printed out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ef962-2760-4885-8f1c-ff25bc6ade5b",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366885fe-f0d9-4bc3-a52d-ec4dff47813f",
   "metadata": {},
   "source": [
    "Evaluate the model's performance using several metrics: Accuracy, Precision, Recall, F1 Score, Confusion Matrix, and AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa25d8-af36-4958-9ade-6b27d6fdff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Precision, Recall, F1-Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Precision (weighted): {precision}')\n",
    "print(f'Recall (weighted): {recall}')\n",
    "print(f'F1 Score (weighted): {f1}')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# AUC-ROC\n",
    "y_pred_prob = clf.predict_proba(X_test_embeddings)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
    "print(f'AUC-ROC: {auc_roc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033d0c9-7699-4764-a691-1ea29228945f",
   "metadata": {},
   "source": [
    "- Metrics computed to evaluate the performance of the model.\n",
    "- The confusion matrix serves as visual to see how the model is performing.\n",
    "- We calculate the AUC-ROC score for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc7649-42f1-430c-b423-48ecb36b007d",
   "metadata": {},
   "source": [
    "# Insights and Future Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a08b6-e7e1-4fdc-82e4-4516326182b2",
   "metadata": {},
   "source": [
    "Instead of relying on a train-test split, one might consider using a train-validation-test split, where validation set is used for model selection and hyperparameter tuning. Cross-validation can help the model generalize across different subsets of the data.\n",
    "\n",
    "If some stopwords (e.g., technical terms or common newsgroup jargon) are important for classification, one can create a custom stopword list that includes domain-specific stopwords. This allows for a more fine-tuned preprocess.\n",
    "\n",
    "In some cases, rare words might contain valuable information, especially in niche categories. Word2Vec's min_count=5 parameter filters out rare words, but one might experiment with lowering or increasing this threshold depending on the dataset.\n",
    "\n",
    "If one observes that some categories are underrepresented in the dataset, one can use techniques like SVM or logistic regression to give more importance to minority classes during training. This can help improve performance for underrepresented categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3826cc2-5dcf-444a-b269-b75cd22627c0",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc9e14-45b0-4a58-b4e0-ab735a1e1329",
   "metadata": {},
   "source": [
    "Bengfort, B., Bilbro, R., & Ojeda, T. (2018). *Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning*. O'Reilly Media, Inc.  \n",
    "Link: [https://www.oreilly.com/library/view/applied-text-analysis/9781491963035/](https://www.oreilly.com/library/view/applied-text-analysis/9781491963035/)\n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2021). *Speech and Language Processing* (3rd ed.). Pearson.  \n",
    "Link: [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "\n",
    "Google Developers. (n.d.). ROC and AUC. *Machine Learning Crash Course*.  \n",
    "Link: [https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "KDnuggets. (2022). Logistic Regression for Classification: Basics and Beyond.  \n",
    "Link: [https://www.kdnuggets.com/2022/04/logistic-regression-classification.html](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)\n",
    "\n",
    "Scikit-learn. (2023). *LogisticRegression*: Scikit-learn Documentation.  \n",
    "Link: [https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html](https://www.scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "Rehurek, R. (2023). Word2Vec Tutorial. *Gensim Documentation*.  \n",
    "Link: [https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
