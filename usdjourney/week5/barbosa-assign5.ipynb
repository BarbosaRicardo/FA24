{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0a8403-5b9f-460c-87ff-65c9e0a0887a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea28f25-d2bd-45b1-99b9-b6bb8cb77975",
   "metadata": {},
   "source": [
    "The dataset will be loaded and preprocessed in a format that the model can work with. We are using the IMDB dataset, which contains 50,000 movie reviews labeled as either positive or negative. The first task is to load this dataset into memory using pandas. To optimize training time on a CPU-based system (working on a 2022 Macbook Pro), we sample a smaller subset and work with TinyBERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bec4c6-ad22-4458-b7ac-911d72474c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# load IMDb dataset\n",
    "file_path = \"/Users/bandito2/Documents/FA24/usdjourney/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# preprocessing the dataset\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# sample an even smaller subset (e.g., 5% of the data) due to slow runtime\n",
    "df = df.sample(frac=0.05, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb59afb-5ccd-4a3e-bf65-e01efbcb2941",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98fc1f-a483-40df-a37c-862a26bc09d3",
   "metadata": {},
   "source": [
    "The raw text of moview reiviews is converted into a format that can be processed by a machine learning model. For transofrmer models like BERT, tokenization involves breaking down sentences into subword units, adding special tokens like [CLS] for classification and [SEP] for separation, and converting words into their corresponding token IDs. BertTokenizer from Hugging Face's Transformers library is used to accomplish this. \n",
    "\n",
    "We use a reduced maximum sequence length of 64 tokens (max_length=64) to further optimize the process, since many reviews are not very long. Truncating longer reviews and padding shorter ones ensures that all input sequences have the same length, which is a requirement for BERT-based models.\n",
    "\n",
    "The tokenize_data function automates this process for both the training and testing datasets, converting the reviews into token IDs, attention masks (to indicate which tokens are real and which are padding), and other inputs required by the TinyBERT model. Efficient tokenization ensures the model can process the data effectively and quickly, especially when working on systems with limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e874d9ae-d202-4ab3-9f72-8f6ca2c7828c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization function with a max sequence length of 64 for faster processing\n",
    "def tokenize_data(data, tokenizer, max_length=64):\n",
    "    return tokenizer(\n",
    "        data['review'].tolist(),\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize the training and test data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_encodings = tokenize_data(train_df, tokenizer, max_length=64)\n",
    "test_encodings = tokenize_data(test_df, tokenizer, max_length=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1751249-53a3-411d-bc00-fc2ed58659e7",
   "metadata": {},
   "source": [
    "# DataLoader Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fead57-4c78-46cd-96a0-cd552092510f",
   "metadata": {},
   "source": [
    "Once the data has been tokenized, the next step is to load it into a format suitable for training the model. PyTorch’s TensorDataset and DataLoader classes are used for this purpose. TensorDataset takes the tokenized inputs and corresponding sentiment labels (0 for negative, 1 for positive) and packages them together. This dataset can then be passed to a DataLoader, which handles batching, shuffling, and feeding the data to the model during training.\n",
    "\n",
    "The batch size is set to 4 in this example to optimize for memory usage and computational speed, particularly when running on a CPU without GPU acceleration due to working with macOS. A smaller batch size means fewer data points are processed in parallel, but this is a reasonable trade-off when working on systems with limited resources.\n",
    "\n",
    "The train_dataloader and test_dataloader are created for the training and testing datasets, respectively. By setting shuffle=True in the train_dataloader, we ensure that the model doesn’t learn any sequence-dependent patterns from the order of the data, which could lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1d2899-b429-488f-9254-1de2922b40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# convert labels into tensors\n",
    "train_labels = torch.tensor(train_df['label'].values)\n",
    "test_labels = torch.tensor(test_df['label'].values)\n",
    "\n",
    "# create DataLoader for training and testing with a smaller batch size\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660197a9-fa6a-4bbd-874b-ab8a144f031f",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b92a6e-d80e-4239-9a9a-08f8e9195d81",
   "metadata": {},
   "source": [
    "In this step, we initialize the pre-trained TinyBERT model, which has been designed for lightweight tasks while retaining much of BERT's power. Hugging Face’s BertForSequenceClassification is used to load a model for classification. Since we are dealing with a binary classification problem (positive or negative sentiment), we specify num_labels=2.\n",
    "\n",
    "TinyBERT is a much smaller version of BERT, which makes it suited for running on CPU-based systems. We initialize the model with pre-trained weights from the TinyBERT_General_6L_768D checkpoint, ensuring that the model has a strong understanding of general language representations before fine-tuning on our IMDb dataset.\n",
    "\n",
    "The optimizer we use is AdamW, which is a variant of the Adam optimizer that involves weight decay, helping to prevent overfitting by penalizing large weights. The model is set to run on the CPU, which is the default device for macOS systems without CUDA support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07342041-a99e-43ee-9397-f3df4589aa84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# load the pre-trained TinyBERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_6L_768D', num_labels=2)\n",
    "\n",
    "# optimizer set up\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47499db1-4198-4c3d-add4-3107f769bd01",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545e65d-4f12-431f-9b5e-556879515d7d",
   "metadata": {},
   "source": [
    "The train_model function handles the training by iterating over the training data for a specified number of epochs (in this case, 1 epoch for faster processing). During each iteration, the model performs a forward pass on the input data to compute predictions, compares the predictions to the true labels, and calculates the loss. The loss function used is cross-entropy, which is common for binary classification tasks.\n",
    "\n",
    "The optimizer (AdamW) is used to adjust the model’s weights based on the computed loss. Before each backward pass, we call optimizer.zero_grad() to ensure that the gradients are not accumulated across batches. Once the backward pass is completed, we call optimizer.step() to update the model's weights based on the computed gradients.\n",
    "\n",
    "This loop is run for 1 epoch, which is sufficient for a quick experiment, but more epochs could be added for better performance at the cost of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32de634-8d7d-4809-ba0a-00b61d79f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5550506313890219\n",
      "Epoch 2, Loss: 0.4047524764947593\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "def train_model(model, train_dataloader, optimizer, device):\n",
    "    model.train()  # set the model to training mode\n",
    "    for epoch in range(2):  # train for 2 epochs for faster results\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            # move batch data to the specified device (CPU in this case)\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # clear previous gradients before computing new ones\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass: pass input data through the model and get the outputs\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "            loss = outputs.loss  # extract the loss from the outputs\n",
    "            \n",
    "            total_loss += loss.item()  # add the loss of this batch to the total loss\n",
    "            \n",
    "            # backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # update model weights based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        # print average loss for this epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# call the training function to fine-tune the model\n",
    "train_model(model, train_dataloader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f6f0e-1c0a-4b17-87d0-7e20e2c3bd0c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8716d90-d75f-4ae6-8636-0c6581978ecc",
   "metadata": {},
   "source": [
    "After training, it's essential to evaluate the model’s performance on the test dataset, to gauge how well the model generalizes to unseen data. The evaluate_model function handles this by performing a forward pass on the test data without updating the model’s weights (hence the use of torch.no_grad() to disable gradient computation).\n",
    "\n",
    "For each batch of test data, the model predicts the sentiment labels, and these predictions are compared with the actual labels. Performance metrics such as accuracy, precision, recall, and F1-score are computed to measure the model’s effectiveness. Accuracy tells us the percentage of correct predictions, while precision, recall, and F1-score provide more nuanced insights into the model’s performance, especially when dealing with imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c3021a-b84f-467c-b1d3-be3c3e1d1103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.764\n",
      "Precision: 0.762962962962963, Recall: 0.7923076923076923, F1-Score: 0.7773584905660378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = [b.to(device) for b in batch]\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141f94b-2f3e-4e46-b1b6-03507c78a8f1",
   "metadata": {},
   "source": [
    "The evaluation function computes the model’s performance metrics using the test dataset. In this step, we use PyTorch to make predictions on the test data without modifying the model parameters. For each batch in the test dataloader, the model outputs predicted logits (unnormalized scores for each class), which are then converted to class labels (0 or 1). These predicted labels are compared to the true labels to compute the accuracy, precision, recall, and F1-score. Precision measures the proportion of positive predictions that are actually positive, recall measures the proportion of actual positives that were correctly identified, and F1-score provides a harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cb805-e9d6-4bbf-8ce9-c3b636dd27b2",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74aa57-862b-4965-b2ec-d85e1eb3580f",
   "metadata": {},
   "source": [
    "It is useful to see how the model performs on individual samples. The predict_sentiment function allows us to input a custom movie review and have the model predict its sentiment. The function tokenizes the input text into token IDs, passes these IDs through the model, and outputs a predicted label (either positive or negative sentiment).\n",
    "\n",
    "This function is a practical way to see the model in action, demonstrating its ability to classify real-world data. This can be particularly useful for tasks like customer feedback analysis, where the goal is to quickly determine the sentiment of a large number of textual inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931300a1-6505-4bdf-96c2-6b462ee53b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# sample prediction\n",
    "def predict_sentiment(review, model, tokenizer, device, max_length=64):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(review, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# example prediction\n",
    "review = \"This movie was absolutely fantastic!\"\n",
    "sentiment = predict_sentiment(review, model, tokenizer, device)\n",
    "print(f\"Predicted sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510991c-cff6-4b9f-bb5a-107b9d28c29b",
   "metadata": {},
   "source": [
    "The predict_sentiment function tokenizes a single review and predicts its sentiment. The model is set to evaluation mode (model.eval()), and torch.no_grad() ensures that no unnecessary gradients are computed during prediction. The model outputs logits for each class, and the class with the highest score is selected as the predicted label. The predicted label (0 for negative, 1 for positive) is converted into human-readable sentiment (\"Positive\" or \"Negative\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1d2ab-c7a7-443e-9ca4-fe634d4418ec",
   "metadata": {},
   "source": [
    "# Conclusion and Future Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338efbe-d6f2-4628-b536-f6186a4bf33d",
   "metadata": {},
   "source": [
    "A sentiment analysis model using TinyBERT to classify IMDb movie reviews as positive or negative was implemented. We optimized the process to run efficiently on a CPU-based system like macOS by reducing the dataset size, limiting the sequence length, and minimizing the number of training epochs. By leveraging transformer-based models like TinyBERT, we achieved strong performance in understanding and classifying sentiments. The evaluation metrics, including accuracy, precision, recall, and F1-score, allowed us to assess the model’s performance on the test dataset. This approach is scalable, adaptable, and highly useful for real-world applications such as review analysis, customer feedback monitoring, and content moderation.\n",
    "\n",
    "While the model performs okay for sentiment classification, there are several areas for future improvements. First, we could explore hyperparameter tuning to optimize the learning rate, batch size, and number of epochs, which could further improve the model’s accuracy and efficiency. Additionally, employing transfer learning by fine-tuning larger models like BERT or RoBERTa on a larger dataset could provide better performance. Another future improvement could involve exploring multi-class sentiment analysis (e.g., very negative, negative, neutral, positive, very positive) for more nuanced understanding. Furthermore, using distillation techniques to reduce the size of models while maintaining performance could make them more practical for deployment on edge devices or mobile platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b147d-e7bb-45b4-85bf-7d8519154074",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41d39e-0c6f-49b0-aa8b-23c0c4094647",
   "metadata": {},
   "source": [
    "Jalammar, J. (2018). *The illustrated transformer*. Retrieved from http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2024). *Speech and Language Processing* (3rd ed.). Draft of February 2024. PDF File: Transformers and Large Language Models.\n",
    "\n",
    "Tunstall, L., von Werra, L., & Wolf, T. (2022). *Natural Language Processing with Transformers: Building Language Applications with Hugging Face*. O'Reilly Media. PDF File: Natural Language Processing with Transformers.\n",
    "\n",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). *Attention is all you need*. Advances in Neural Information Processing Systems, 30, 5998-6008.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Ricardo Barbosa"
   }
  ],
  "date": "September 30, 2024",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "title": "AAI-520 Assignment 5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
