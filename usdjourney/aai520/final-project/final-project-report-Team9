{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a06f7a-3985-426b-9038-0124aeeb351a",
   "metadata": {},
   "source": [
    "# Research and Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbcfb3-5d6a-4c11-b588-84fb99e49ef3",
   "metadata": {},
   "source": [
    "The first step was to understand various architectures used in chatbot development. We explored Seq2Seq, Transformers, and GPT models in this course:\n",
    "\n",
    "Seq2Seq models encode the input sequence and generate the output sequence. However, they struggle with long-term dependencies because the model tends to forget the earlier parts of conversations.\n",
    "Transformers, on the other hand, introduced the self-attention mechanism, which allows the model to focus on relevant parts of the input sequence, thus handling long-range dependencies better ( Lewis Tunstall)\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is a Transformer-based model that excels at generating human-like text and handling multi-turn dialogues due to its ability to manage long-term context using its pre-trained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b77fd0-091c-4dc2-8660-9c0492210de3",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf8556-036a-48b1-bc7a-04a2348a5853",
   "metadata": {},
   "source": [
    "The Cornell Movie Dialogs Corpus was selected for training. It contains over 220,000 conversational exchanges from movies​(README). The preprocessing phase involved tokenizing the dialogues, managing conversation turns, and cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf11e8-7c71-4478-9797-6ed4e45994d5",
   "metadata": {},
   "source": [
    "## Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187b6ce5-2440-48dd-82fa-3d1bd3792018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the Cornell Movie Dialogs Corpus\n",
    "movie_lines_file = \"/Users/bandito2/Documents/FA24/usdjourney/aai520/final-project/archive/movie_lines.txt\"\n",
    "movie_conversations_file = \"/Users/bandito2/Documents/FA24/usdjourney/aai520/final-project/archive/movie_conversations.txt\"\n",
    "\n",
    "# Function to load movie lines\n",
    "def load_movie_lines(file_path):\n",
    "    lines = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\" +++$+++ \")\n",
    "                if len(parts) == 5:\n",
    "                    lines[parts[0]] = parts[4]  # Line ID -> Dialogue text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return lines\n",
    "\n",
    "# Function to load conversations\n",
    "def load_conversations(file_path, lines_dict):\n",
    "    conversations = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\" +++$+++ \")\n",
    "                if len(parts) == 4:\n",
    "                    utterance_ids = eval(parts[-1])  # Convert string list to actual list\n",
    "                    conversation = [lines_dict.get(utterance_id, \"\") for utterance_id in utterance_ids if utterance_id in lines_dict]\n",
    "                    if conversation and all(conversation):\n",
    "                        conversations.append(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return conversations\n",
    "\n",
    "# Load movie lines and conversations\n",
    "lines_dict = load_movie_lines(movie_lines_file)\n",
    "conversations = load_conversations(movie_conversations_file, lines_dict)\n",
    "\n",
    "# Example: Print first conversation\n",
    "print(conversations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca60cc9-fa56-4c20-a20d-cf5dee88d3d4",
   "metadata": {},
   "source": [
    "## Code for Tokenization and Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29bf0bb2-3a0d-4167-9c0d-3dbd39bfe327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([15496,    11,   703,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is set to EOS token for GPT-2\n",
    "\n",
    "# Custom Dataset class for GPT-2\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, conversations, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Tokenize conversations and return input_ids and attention_mask\n",
    "        self.encodings = tokenizer(\n",
    "            conversations,\n",
    "            padding=\"max_length\",        # Pad sequences to max_length\n",
    "            truncation=True,             # Truncate sequences longer than max_length\n",
    "            max_length=self.max_length,  # Max length for GPT-2 is 1024 tokens\n",
    "            return_tensors=\"pt\"          # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return input_ids and attention_mask for each item\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Example conversations\n",
    "conversations = [\"Hello, how are you?\", \"I'm doing well, thank you!\", \"What about you?\"]\n",
    "\n",
    "# Prepare dataset\n",
    "chatbot_dataset = ChatbotDataset(conversations, tokenizer)\n",
    "\n",
    "# Check the dataset\n",
    "print(chatbot_dataset[0])  # Should return input_ids and attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cb09c-6800-48c4-903f-071e00e71164",
   "metadata": {},
   "source": [
    "Tokenization: We used the GPT-2 tokenizer to convert text into tokens that the model understands.\n",
    "Handling Multi-Turn Conversations: We prepared conversations by grouping exchanges so that the model could learn context across multiple dialogue turns​( Lewis Tunstall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702881c-3deb-4e84-a362-561f4d0b15a8",
   "metadata": {},
   "source": [
    "# Model Design and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbbf29-806c-454f-9d83-092c6539b0dd",
   "metadata": {},
   "source": [
    "In this phase, we fine-tuned the GPT-2 model on the tokenized data from the Cornell Movie Dialogs Corpus. Fine-tuning the pre-trained model on conversational data helps it adapt to the dialogue structure and style, making it capable of generating contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f9380-b263-4d95-9de0-60151d992d2b",
   "metadata": {},
   "source": [
    "# Prepare Dataset with Labels for GTP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8fb52f5-cd69-4a98-acdf-352e5de0dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   39, 50256, 50256,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 0, 0,  ..., 0, 0, 0]), 'labels': tensor([   39, 50256, 50256,  ..., 50256, 50256, 50256])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class for GPT-2 with labels\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "        # Labels are the same as input_ids for GPT-2\n",
    "        self.labels = self.input_ids.clone()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "# Prepare the dataset with the tokenized data\n",
    "chatbot_dataset = ChatbotDataset(tokenized_conversations)\n",
    "\n",
    "# Check the first item to ensure it contains input_ids, attention_mask, and labels\n",
    "print(chatbot_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33c93a-0d87-4eab-b342-d289b918cee5",
   "metadata": {},
   "source": [
    "## Fine-Tuning Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d628e2a-1bbc-4d5c-82a3-b6982fc11430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/90 28:30 < 1:01:05, 0.02 it/s, Epoch 0.97/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,    # Batch size per device\n",
    "    logging_dir='./logs',             # Directory for logs\n",
    "    logging_steps=100,                # Log every 100 steps\n",
    "    save_steps=500,                   # Save model every 500 steps\n",
    "    save_total_limit=2,               # Limit the number of saved checkpoints\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=chatbot_dataset,    # Use the custom dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40f706-5aa7-4c7d-af57-a9708add1a70",
   "metadata": {},
   "source": [
    "The Hugging Face Trainer is used to manage the training loop, handling forward passes, loss calculation, and backpropagation​( Lewis Tunstall).\n",
    "\n",
    "The GPT-2 model was fine-tuned for three epochs with a batch size of 2, allowing the model to adjust its weights based on the conversation data, making it proficient in dialogue generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1148d-6571-417f-8dfc-864f69f16397",
   "metadata": {},
   "source": [
    "# Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd14ab-6e0f-4a6e-859c-374cbbd67d39",
   "metadata": {},
   "source": [
    "After training, the chatbot was evaluated both interactively and through quantitative metrics such as perplexity. Perplexity is a common metric used to evaluate language models, where a lower perplexity indicates better performance in predicting the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d96f8d-c46e-4b75-a910-f3cc5c7141a3",
   "metadata": {},
   "source": [
    "## Perplexity Calculation Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1033c-3826-486c-9833-96ada28d25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "# Test on a sample sentence\n",
    "sample_text = \"Hello, how are you today?\"\n",
    "perplexity = calculate_perplexity(sample_text)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243278d0-3346-4a7f-95e2-d93b177dcab4",
   "metadata": {},
   "source": [
    "Perplexity measures how well the model predicts the next token in a sequence. A lower value indicates that the model is more accurate and confident in its predictions (Lewis Tunstall).\n",
    "\n",
    "Interactive Testing: We also tested the chatbot interactively by providing various conversational prompts and analyzing how well the model handles context over multiple turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e655d22-fe09-4be3-95e6-eefe313b4b6a",
   "metadata": {},
   "source": [
    "# Building a Web Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1374e8-dc84-4d95-ab03-fd94cf474a9e",
   "metadata": {},
   "source": [
    "We used Flask to build a simple web interface, allowing users to interact with the chatbot in real-time. The web application takes user input, sends it to the chatbot, and displays the response on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c50db-e155-4b83-a7bd-9d78e7e25ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def generate_response(user_input):\n",
    "    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=200, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "    return tokenizer.decode(outputs[:, inputs.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.form['user_input']\n",
    "    bot_response = generate_response(user_input)\n",
    "    return {'response': bot_response}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7359312-20c7-4cb3-9bca-6cac0b42ccf7",
   "metadata": {},
   "source": [
    "Flask is a Python web framework was used to create the user interface. The chatbot generates a response in real-time based on user input and returns it to the webpage.\n",
    "\n",
    "Web Interface allows users to interact with the chatbot in a more accessible and user-friendly environment (Lewis Tunstall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddf799-d3e1-4c58-8634-3f790b59a93f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b69186-7608-473f-adcb-94161f6e0306",
   "metadata": {},
   "source": [
    "This project demonstrated how to build a generative-based chatbot using GPT-2, from data preprocessing to model training, evaluation, and deployment. We explored the key steps in creating a multi-turn conversational chatbot, including the challenges of managing context and generating coherent responses. Future improvements could include training on more diverse datasets, refining conversational abilities, and deploying the chatbot in a scalable environment like AWS or Heroku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
