{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a06f7a-3985-426b-9038-0124aeeb351a",
   "metadata": {},
   "source": [
    "# Research and Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbcfb3-5d6a-4c11-b588-84fb99e49ef3",
   "metadata": {},
   "source": [
    "The first step was to understand various architectures used in chatbot development. We explored Seq2Seq, Transformers, and GPT models in this course:\n",
    "\n",
    "Seq2Seq models encode the input sequence and generate the output sequence. However, they struggle with long-term dependencies because the model tends to forget the earlier parts of conversations.\n",
    "Transformers, on the other hand, introduced the self-attention mechanism, which allows the model to focus on relevant parts of the input sequence, thus handling long-range dependencies better ( Lewis Tunstall)\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is a Transformer-based model that excels at generating human-like text and handling multi-turn dialogues due to its ability to manage long-term context using its pre-trained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b77fd0-091c-4dc2-8660-9c0492210de3",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf8556-036a-48b1-bc7a-04a2348a5853",
   "metadata": {},
   "source": [
    "The Cornell Movie Dialogs Corpus was selected for training. It contains over 220,000 conversational exchanges from movies​(README). The preprocessing phase involved tokenizing the dialogues, managing conversation turns, and cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf11e8-7c71-4478-9797-6ed4e45994d5",
   "metadata": {},
   "source": [
    "## Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187b6ce5-2440-48dd-82fa-3d1bd3792018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the Cornell Movie Dialogs Corpus\n",
    "movie_lines_file = \"/Users/bandito2/Documents/FA24/usdjourney/aai520/final-project/archive/movie_lines.txt\"\n",
    "movie_conversations_file = \"/Users/bandito2/Documents/FA24/usdjourney/aai520/final-project/archive/movie_conversations.txt\"\n",
    "\n",
    "# Function to load movie lines\n",
    "def load_movie_lines(file_path):\n",
    "    lines = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\" +++$+++ \")\n",
    "                if len(parts) == 5:\n",
    "                    lines[parts[0]] = parts[4]  # Line ID -> Dialogue text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return lines\n",
    "\n",
    "# Function to load conversations\n",
    "def load_conversations(file_path, lines_dict):\n",
    "    conversations = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\" +++$+++ \")\n",
    "                if len(parts) == 4:\n",
    "                    utterance_ids = eval(parts[-1])  # Convert string list to actual list\n",
    "                    conversation = [lines_dict.get(utterance_id, \"\") for utterance_id in utterance_ids if utterance_id in lines_dict]\n",
    "                    if conversation and all(conversation):\n",
    "                        conversations.append(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return conversations\n",
    "\n",
    "# Load movie lines and conversations\n",
    "lines_dict = load_movie_lines(movie_lines_file)\n",
    "conversations = load_conversations(movie_conversations_file, lines_dict)\n",
    "\n",
    "# Example: Print first conversation\n",
    "print(conversations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca60cc9-fa56-4c20-a20d-cf5dee88d3d4",
   "metadata": {},
   "source": [
    "## Code for Tokenization and Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29bf0bb2-3a0d-4167-9c0d-3dbd39bfe327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   39, 50256, 50256,  ..., 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as the padding token\n",
    "\n",
    "# Tokenize conversations and pad to max length\n",
    "def tokenize_conversations(conversations, tokenizer, max_length=1024):\n",
    "    tokenized_conversations = []\n",
    "    for conversation in conversations:\n",
    "        for sentence in conversation:\n",
    "            # Tokenize and truncate sentences exceeding max_length\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "            tokenized_conversations.append(tokens)\n",
    "\n",
    "    # Pad all sequences to max_length\n",
    "    padded_conversations = tokenizer.pad(\n",
    "        {\"input_ids\": tokenized_conversations},\n",
    "        padding=\"max_length\",    # Pad sequences to max_length\n",
    "        max_length=max_length,   # Ensure all sequences are exactly max_length\n",
    "        return_tensors=\"pt\"      # Return as PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    return padded_conversations\n",
    "\n",
    "# Tokenize and pad the conversations\n",
    "tokenized_conversations = tokenize_conversations(conversations, tokenizer)\n",
    "\n",
    "# Check a sample of the tokenized and padded conversations\n",
    "print(tokenized_conversations['input_ids'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cb09c-6800-48c4-903f-071e00e71164",
   "metadata": {},
   "source": [
    "Tokenization: We used the GPT-2 tokenizer to convert text into tokens that the model understands.\n",
    "Handling Multi-Turn Conversations: We prepared conversations by grouping exchanges so that the model could learn context across multiple dialogue turns​( Lewis Tunstall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702881c-3deb-4e84-a362-561f4d0b15a8",
   "metadata": {},
   "source": [
    "# Model Design and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbbf29-806c-454f-9d83-092c6539b0dd",
   "metadata": {},
   "source": [
    "In this phase, we fine-tuned the GPT-2 model on the tokenized data from the Cornell Movie Dialogs Corpus. Fine-tuning the pre-trained model on conversational data helps it adapt to the dialogue structure and style, making it capable of generating contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f9380-b263-4d95-9de0-60151d992d2b",
   "metadata": {},
   "source": [
    "# Prepare Dataset with Labels for GTP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8fb52f5-cd69-4a98-acdf-352e5de0dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   39, 50256, 50256,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 0, 0,  ..., 0, 0, 0]), 'labels': tensor([   39, 50256, 50256,  ..., 50256, 50256, 50256])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class for GPT-2 with labels\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "        # Labels are the same as input_ids for GPT-2\n",
    "        self.labels = self.input_ids.clone()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "# Prepare the dataset with the tokenized data\n",
    "chatbot_dataset = ChatbotDataset(tokenized_conversations)\n",
    "\n",
    "# Check the first item to ensure it contains input_ids, attention_mask, and labels\n",
    "print(chatbot_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33c93a-0d87-4eab-b342-d289b918cee5",
   "metadata": {},
   "source": [
    "## Fine-Tuning Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d628e2a-1bbc-4d5c-82a3-b6982fc11430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,    # Batch size per device\n",
    "    logging_dir='./logs',             # Directory for logs\n",
    "    logging_steps=100,                # Log every 100 steps\n",
    "    save_steps=500,                   # Save model every 500 steps\n",
    "    save_total_limit=2,               # Limit the number of saved checkpoints\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=chatbot_dataset,    # Use the custom dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40f706-5aa7-4c7d-af57-a9708add1a70",
   "metadata": {},
   "source": [
    "The Hugging Face Trainer is used to manage the training loop, handling forward passes, loss calculation, and backpropagation​( Lewis Tunstall).\n",
    "\n",
    "The GPT-2 model was fine-tuned for three epochs with a batch size of 2, allowing the model to adjust its weights based on the conversation data, making it proficient in dialogue generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1148d-6571-417f-8dfc-864f69f16397",
   "metadata": {},
   "source": [
    "# Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd14ab-6e0f-4a6e-859c-374cbbd67d39",
   "metadata": {},
   "source": [
    "After training, the chatbot was evaluated both interactively and through quantitative metrics such as perplexity. Perplexity is a common metric used to evaluate language models, where a lower perplexity indicates better performance in predicting the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d96f8d-c46e-4b75-a910-f3cc5c7141a3",
   "metadata": {},
   "source": [
    "## Perplexity Calculation Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1033c-3826-486c-9833-96ada28d25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Check if MPS (Apple GPU) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Move the model to MPS device\n",
    "model.to(device)\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)  # Ensure inputs are on MPS\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "# Example text\n",
    "sample_text = \"How have you been doing since we last spoke?\"\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = calculate_perplexity(sample_text)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243278d0-3346-4a7f-95e2-d93b177dcab4",
   "metadata": {},
   "source": [
    "Perplexity measures how well the model predicts the next token in a sequence. A lower value indicates that the model is more accurate and confident in its predictions (Lewis Tunstall).\n",
    "\n",
    "Interactive Testing: We also tested the chatbot interactively by providing various conversational prompts and analyzing how well the model handles context over multiple turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e655d22-fe09-4be3-95e6-eefe313b4b6a",
   "metadata": {},
   "source": [
    "# Building a Web Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1374e8-dc84-4d95-ab03-fd94cf474a9e",
   "metadata": {},
   "source": [
    "We used Flask to build a simple web interface, allowing users to interact with the chatbot in real-time. The web application takes user input, sends it to the chatbot, and displays the response on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7359312-20c7-4cb3-9bca-6cac0b42ccf7",
   "metadata": {},
   "source": [
    "Flask is a Python web framework was used to create the user interface. The chatbot generates a response in real-time based on user input and returns it to the webpage.\n",
    "\n",
    "Web Interface allows users to interact with the chatbot in a more accessible and user-friendly environment (Lewis Tunstall )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddf799-d3e1-4c58-8634-3f790b59a93f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b69186-7608-473f-adcb-94161f6e0306",
   "metadata": {},
   "source": [
    "This project demonstrated how to build a generative-based chatbot using GPT-2, from data preprocessing to model training, evaluation, and deployment. We explored the key steps in creating a multi-turn conversational chatbot, including the challenges of managing context and generating coherent responses. Future improvements could include training on more diverse datasets, refining conversational abilities, and deploying the chatbot in a scalable environment like AWS or Heroku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
