{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168e14fa-df5f-4781-a7eb-d49fa077f53a",
   "metadata": {},
   "source": [
    "# Dataset Importing and Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985469a9-235d-4a6c-8a50-ef078b40f29f",
   "metadata": {},
   "source": [
    "We will preprocess the IMDb dataset, containing 50,000 movie reviews split into training and testing datasets. We will be utilizing BERT’s tokenizer to transform the text into tokens that the BERT model can understand. The tokenization process breaks the text into small units (subwords) that the model can process while ensuring that the input sequence fits within BERT’s token limit of 512 tokens. The tokenizer also adds special tokens like [CLS] for classification tasks and [SEP] to separate input sentences.\n",
    "\n",
    "The Hugging Face library’s BertTokenizer for subword tokenization will be used, which is  efficient for handling out-of-vocabulary words, as described in the Natural Language Processing with Transformers (Tunstall et al., 2022) and the Illustrated Transformer blog (Jalammar, 2018). This step helps prepare the dataset for training by ensuring the input format is compatible with BERT’s architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0217a563-f584-46c4-ae3f-f7e057dd9bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# load the IMDb dataset\n",
    "file_path = \"/Users/bandito2/Documents/FA24/usdjourney/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head()) # sanity check\n",
    "\n",
    "# split the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenize data in dataset\n",
    "def tokenize_data(data, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        data['review'].values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt', \n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df, tokenizer)\n",
    "test_encodings = tokenize_data(test_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f6efd-d004-42e6-ae8a-2af6a062d037",
   "metadata": {},
   "source": [
    "We split the data into training and testing sets and tokenizes the movie reviews using BERT’s tokenizer. As described by Vaswani et al. (2017) in the Attention Is All You Need paper, the tokenizer divides the input text into subwords and ensures the input format is compatible with BERT’s self-attention mechanism. The subword tokenization strategy ensures that even out-of-vocabulary words can be processed effectively by breaking them into smaller subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18268da9-ac3a-4e9a-a167-3eefb125add9",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5f923-35b7-40e1-8ca8-f0d67ca7a171",
   "metadata": {},
   "source": [
    "BERT is a transformer model pre-trained on large amount of text, making it ideal for fine-tuning on specific tasks like sentiment analysis. After preprocessing, we load the pre-trained BERT model and fine-tune it on the IMDb dataset using BertForSequenceClassification. Fine-tuning BERT involves updating its weights based on the task at hand, which in this case is binary classification (positive or negative review).\n",
    "\n",
    "During training, we use the AdamW optimizer, which is designed for training transformer models, as discussed in the Illustrated Transformer (Jalammar, 2018). We also leverage the cross-entropy loss function, which is used for classification. The training process involves multiple epochs of forward and backward passes, where the model learns to predict sentiments based on the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2f8f8-ec54-4bec-8a5b-0eeb2bbd5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# Specify the device as CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Convert encodings to dataset\n",
    "labels = torch.tensor(train_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values)\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], labels)\n",
    "\n",
    "# Load the pre-trained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)  # Ensure the model is running on the CPU\n",
    "\n",
    "# Create a DataLoader for the training set with a smaller batch size (8 instead of 16)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Specify the number of epochs\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Move data to the CPU\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        model.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the average loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}: Loss {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531d57c-5dc2-4e0b-9ebd-4a7a0e461fda",
   "metadata": {},
   "source": [
    "Here, we fine-tune BERT using the IMDb dataset. Fine-tuning involves updating the pre-trained weights of BERT for the specific sentiment classification task. We use the AdamW optimizer, which is suited for handling the complexities of transformer models, and cross-entropy as the loss function for binary classification. This training process allows BERT to learn from the IMDb reviews and adjust its weights to perform well on this sentiment classification task, as discussed by Tunstall et al. (2022) in their chapter on text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1584da9-7002-4840-b4aa-b89233a5b6a5",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec82735-24ff-436c-8d9c-3905f79991fb",
   "metadata": {},
   "source": [
    "Evaluating the model's performance involves computing accuracy, precision, recall, and F1-score, which are common metrics used in binary classification tasks. After training the model, we test its performance on the test set. The classification_report from sklearn generates a detailed breakdown of these metrics.\n",
    "\n",
    "As emphasized in Transformers and Large Language Models (Jurafsky & Martin, 2024), evaluating a model's performance on unseen data is critical for understanding how well the model generalizes to new data. Using multiple metrics ensures that we don’t just measure overall accuracy, but also how well the model handles false positives and false negatives.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327eaf4-9832-4d0f-a140-942cd605a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# prepare the test set\n",
    "test_labels = torch.tensor(test_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# evaluation loop\n",
    "for batch in test_dataloader:\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = tuple(b.to('cuda') for b in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_attention_masks)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "    true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# generate evaluation metrics\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000af5b-f8f9-47d4-8bb0-c8bc869a375a",
   "metadata": {},
   "source": [
    "The evaluation process compares the model's predictions on the test set against the true labels. We use precision, recall, and F1-score to provide a comprehensive view of the model’s performance. F1-score is especially important for handling imbalanced datasets. The evaluation metrics provide insight into where the model is performing well and where improvements can be made. As described by Jurafsky and Martin (2024), evaluating a model’s performance on test data ensures the model is not overfitting and can generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b00577-1c45-4570-baf4-b4831129b1ff",
   "metadata": {},
   "source": [
    "# Predictions and Sample Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2411e88-8819-4d41-b60f-01a6f0ec0db8",
   "metadata": {},
   "source": [
    "In this step, we use the fine-tuned model to make predictions on new movie reviews. This demonstrates the model’s practical application in predicting sentiments. After tokenizing the input, we pass the text through the model to obtain predictions. The attention mechanism within BERT ensures that the model focuses on the most relevant parts of the input text to predict the sentiment, as described in the Illustrated Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e432a5a-1a87-4080-80b3-1ba53194ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"The movie was absolutely fantastic. I loved the acting and the story!\",\n",
    "    \"This was the worst film I have ever seen. It was a waste of time.\"\n",
    "]\n",
    "\n",
    "# Tokenize sample reviews\n",
    "sample_encodings = tokenizer.batch_encode_plus(\n",
    "    sample_reviews, add_special_tokens=True, return_attention_mask=True, pad_to_max_length=True, max_length=512, return_tensors='pt'\n",
    ")\n",
    "\n",
    "sample_input_ids = sample_encodings['input_ids'].to('cuda')\n",
    "sample_attention_masks = sample_encodings['attention_mask'].to('cuda')\n",
    "\n",
    "# Predict sentiments\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_input_ids, token_type_ids=None, attention_mask=sample_attention_masks)\n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Display results\n",
    "for review, pred in zip(sample_reviews, preds):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted sentiment: {'Positive' if pred == 1 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d66f6-00a9-4ac2-b85c-468946074435",
   "metadata": {},
   "source": [
    "This step illustrates how to apply the model to unseen data. The BERT model, using self-attention, captures long-range dependencies in the text, helping it focus on important words that determine sentiment. For example, in the sentence “The movie was fantastic”, BERT can focus on the word “fantastic” to determine that the sentiment is positive. This aligns with the attention mechanism discussed in the Illustrated Transformer blog (Jalammar, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4a2a8-6dd9-47fc-932b-423f083132ea",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "While the current implementation of BERT for sentiment analysis on the IMDb dataset yields promising results, there are several avenues for future improvements and exploration. First, newer transformer models such as RoBERTa, ALBERT, and DistilBERT offer potential improvements in terms of accuracy and efficiency, as discussed in *Natural Language Processing with Transformers* (Tunstall et al., 2022). These models have slight architectural modifications that may provide better performance on sentiment classification tasks.\n",
    "\n",
    "Second, fine-tuning hyperparameters like learning rate, batch size, and number of epochs could be explored using tools such as Optuna for hyperparameter optimization. This could further enhance the model's performance. Another avenue for future research is exploring transfer learning with multilingual models such as XLM-RoBERTa, which would allow sentiment analysis on movie reviews in multiple languages.\n",
    "\n",
    "Moreover, utilizing model interpretability techniques like SHAP or LIME can provide deeper insights into which parts of the text the model focuses on when making predictions. This will help in improving trust and transparency in the model's decision-making process, especially in real-world applications.\n",
    "\n",
    "Lastly, deploying the model on edge devices or low-resource environments using techniques like model pruning, quantization, or knowledge distillation can make the model more efficient and scalable, as noted by *Transformers and Large Language Models* (Jurafsky & Martin, 2024).\n",
    "\n",
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this assignment, we successfully implemented sentiment analysis using the BERT model on the IMDb movie review dataset. Starting with data preprocessing, we utilized BERT’s tokenizer to transform movie reviews into tokens that could be understood by the BERT model. We then fine-tuned a pre-trained BERT model using the IMDb dataset and evaluated its performance based on metrics such as accuracy, precision, recall, and F1-score. \n",
    "\n",
    "BERT’s self-attention mechanism, as discussed in the *Illustrated Transformer* blog (Jalammar, 2018), allowed the model to capture long-range dependencies in the text and focus on the most relevant words to predict sentiment. Through predictions on sample reviews, we demonstrated the model's practical application in sentiment analysis.\n",
    "\n",
    "The transformer architecture has revolutionized natural language processing by providing a powerful mechanism for handling a wide range of NLP tasks. This assignment highlights the strength of transfer learning with pre-trained models like BERT and the significant impact it can have on specialized tasks such as sentiment analysis. Moving forward, improvements in model efficiency, interpretability, and scalability can make BERT and other transformer models even more valuable in real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "Jalammar, J. (2018). *The illustrated transformer*. Retrieved from http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2024). *Speech and Language Processing* (3rd ed.). Draft of February 2024.\n",
    "\n",
    "Tunstall, L., von Werra, L., & Wolf, T. (2022). *Natural Language Processing with Transformers: Building Language Applications with Hugging Face*. O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
